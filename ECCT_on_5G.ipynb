{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNh4LEfvblwXz+HmtfXvCA8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pollyjuice74/ECCT/blob/main/ECCT_on_5G.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8YbpCuZ6Yzl2"
      },
      "outputs": [],
      "source": [
        "# ECCT\n",
        "!git clone https://github.com/pollyjuice74/ECCT\n",
        "import os\n",
        "os.chdir('ECCT')\n",
        "\n",
        "from args import pass_args_ecct\n",
        "from Model import *\n",
        "from Codes import *\n",
        "\n",
        "# Enc/Dec 5G\n",
        "!pip install sionna\n",
        "from sionna.fec.ldpc.encoding import LDPC5GEncoder\n",
        "from sionna.utils import BitErrorRate, BinarySource\n",
        "from sionna.mapping import Mapper, Demapper\n",
        "from sionna.channel import AWGN\n",
        "\n",
        "!wget https://raw.githubusercontent.com/pollyjuice74/REU-LDPC-Project/main/5g_enc_dec/decoder.py\n",
        "from decoder import LDPC5GDecoder\n",
        "\n",
        "# Other\n",
        "from torch.nn import functional as F\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import copy\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ECC_Transformer(nn.Module):\n",
        "    def __init__(self, args, encoder, dropout=0):\n",
        "        super(ECC_Transformer, self).__init__()\n",
        "\n",
        "        # code = args.code\n",
        "        c = copy.deepcopy\n",
        "        attn = MultiHeadedAttention(args.h, args.d_model)\n",
        "        ff = PositionwiseFeedForward(args.d_model, args.d_model*4, dropout)\n",
        "\n",
        "        # 5G Compliant encoder/decoder\n",
        "        self.encoder5G = encoder\n",
        "\n",
        "        self.src_embed = torch.nn.Parameter(torch.empty(self.encoder5G._n_ldpc + self.encoder5G.pcm.shape[0], args.d_model)) ### #(code.n + code.pc_matrix.size(0), args.d_model)))\n",
        "\n",
        "        self.decoder = Encoder(EncoderLayer(\n",
        "            args.d_model, c(attn), c(ff), dropout), args.N_dec)\n",
        "\n",
        "        self.oned_final_embed = torch.nn.Sequential(\n",
        "            *[nn.Linear(args.d_model, 1)])\n",
        "\n",
        "        self.out_fc = nn.Linear(self.encoder5G._n_ldpc + self.encoder5G.pcm.shape[0], self.encoder5G._n_ldpc) ###\n",
        "\n",
        "        self.get_mask()\n",
        "        print(f'Mask:\\n {self.src_mask}')\n",
        "\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "\n",
        "    def forward(self, magnitude, syndrome):\n",
        "        emb = torch.cat([magnitude, syndrome], -1).unsqueeze(-1)\n",
        "        emb = self.src_embed.unsqueeze(0) * emb\n",
        "        emb = self.decoder(emb, self.src_mask)\n",
        "        return self.out_fc(self.oned_final_embed(emb).squeeze(-1))\n",
        "\n",
        "\n",
        "    def loss(self, z_pred, z2, y):\n",
        "        loss = F.binary_cross_entropy_with_logits(\n",
        "            z_pred, sign_to_bin(torch.sign(z2)))\n",
        "\n",
        "        x_pred = sign_to_bin(torch.sign(-z_pred * torch.sign(y)))\n",
        "        return loss, x_pred\n",
        "\n",
        "\n",
        "    def get_mask(self, no_mask=False):\n",
        "        if no_mask:\n",
        "            self.src_mask = None\n",
        "            return\n",
        "\n",
        "        mask_size = self.encoder5G._n_ldpc +self.encoder5G.pcm.shape[0] # n + m\n",
        "        src_mask = self.build_mask(mask_size)\n",
        "        # print(src_mask, mask_size)\n",
        "\n",
        "        a = mask_size ** 2\n",
        "        # print(a, )\n",
        "\n",
        "        print(\n",
        "            f'Self-Attention Sparsity Ratio={100 * torch.sum((src_mask).int()) / a:0.2f}%, Self-Attention Complexity Ratio={100 * torch.sum((~src_mask).int())//2 / a:0.2f}%')\n",
        "        self.register_buffer('src_mask', src_mask)\n",
        "\n",
        "\n",
        "    def build_mask(self, mask_size):\n",
        "            mask = torch.eye(mask_size, mask_size)\n",
        "\n",
        "            for ii in range(self.encoder5G.pcm.shape[0]): # m\n",
        "                # edge idxs for cn ii in pcm\n",
        "                idx = self.encoder5G.pcm[ii].indices #torch.where(self.encoder5G.pcm[ii] > 0)[0] #code.pc_matrix[ii]\n",
        "\n",
        "                # print(ii, idx) #self.encoder5G.pcm[ii].indices,\n",
        "                # print(self.encoder5G._n_ldpc, self.encoder5G._k_ldpc)\n",
        "                # print()\n",
        "\n",
        "                for jj in idx:\n",
        "                    for kk in idx:\n",
        "\n",
        "                        # print(mask.shape)\n",
        "                        if jj != kk:\n",
        "                            mask[jj, kk] += 1\n",
        "                            mask[kk, jj] += 1\n",
        "                            mask[self.encoder5G._n_ldpc + ii, jj] += 1\n",
        "                            mask[jj, self.encoder5G._n_ldpc + ii] += 1\n",
        "\n",
        "            src_mask = ~ (mask > 0).unsqueeze(0).unsqueeze(0)\n",
        "            return src_mask\n",
        "\n"
      ],
      "metadata": {
        "id": "9m__yYGWOhTv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sionna.mapping import Mapper, Demapper\n",
        "\n",
        "# Code data\n",
        "k, n = (90, 100)\n",
        "bps = 4 # bits per symbol\n",
        "args = pass_args_ecct()\n",
        "\n",
        "# 5G compliant encoder\n",
        "enc = LDPC5GEncoder(k,n)\n",
        "# Decoder models\n",
        "dec = LDPC5GDecoder(enc)\n",
        "ecct = ECC_Transformer(args, enc)\n",
        "\n",
        "# Message generation\n",
        "binary_source = BinarySource()\n",
        "# Channel objects\n",
        "mapper = Mapper(\"qam\", bps)\n",
        "channel = AWGN()\n",
        "demapper = Demapper(\"app\", \"qam\", bps)\n",
        "\n",
        "\n",
        "def train(model, batch_size=10, iters=100):\n",
        "    model.train()\n",
        "    cum_loss = cum_ber = cum_fer = cum_samples = 0\n",
        "    t = time.time()\n",
        "\n",
        "    for i in range(iters):\n",
        "        b = binary_source([batch_size, enc._k]) # (k,1)\n",
        "        c = enc(b) # (n,1)\n",
        "\n",
        "        # NOISELESS Channel to get (n_ldpc,1) original llrs\n",
        "        x = mapper(c)\n",
        "        llr = demapper([x, no]) # no noise # (n,1)\n",
        "\n",
        "        llr_noiseless, _, _ = dec(llr) # decoder turns (n,1) to (n_ldpc,1)\n",
        "\n",
        "        # AWGN Channel\n",
        "        x = self._mapper(c_pad)\n",
        "        y = self._channel([x, no])\n",
        "        llr_r = self._demapper([y, no])\n",
        "\n",
        "        llr_nldpc, _, _ = dec(llr_r) # decoder turns (n,1) to (n_ldpc,1)\n",
        "\n",
        "        # Model pred from noisy llrs\n",
        "        llr_noiseless_hat = model(llr_nldpc) # (n_ldpc,1)\n",
        "\n",
        "        loss = F.binary_cross_entropy_with_logits(llr_noiseless_hat, llr_noiseless)\n",
        "\n",
        "        # llr to bin, first n values of llr_hat (n_ldpc,1) correspond to c (n,1)\n",
        "        c_hat = (llr_hat[:enc._n] > 0).float()\n",
        "\n",
        "        cum_loss += loss.item() * x.shape[0]\n",
        "        cum_ber += BER(c_hat, c) * x.shape[0]\n",
        "        cum_fer += FER(c_hat, c) * x.shape[0]\n",
        "        cum_samples += x.shape[0]\n",
        "\n",
        "        if i%10 == 0:\n",
        "            print(f'Batch {i + 1}/{iters}: Loss={cum_loss / cum_samples:.2e} BER={cum_ber / cum_samples:.2e} FER={cum_fer / cum_samples:.2e}')\n",
        "\n",
        "    print(f'Train time: {time.time() - t:.2f}s\\n')\n",
        "    return cum_loss / cum_samples, cum_ber / cum_samples, cum_fer / cum_samples\n",
        "\n",
        "\n",
        "def test(model):\n",
        "  pass\n",
        "\n",
        "\n",
        "\n",
        "train(ecct)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        },
        "id": "UGUveWxVNx0T",
        "outputId": "f7368afb-6a6e-4f3d-e2cb-03e1d1424269"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to model/logs: Results_ECCT/POLAR__Code_n_64_k_32__27_06_2024_16_10_59\n",
            "Namespace(epochs=1000, workers=4, lr=0.0001, gpus='-1', batch_size=128, test_batch_size=2048, seed=42, code_type='POLAR', code_k=32, code_n=64, standardize=False, N_dec=6, d_model=32, h=8, code=<args.pass_args_ecct.<locals>.Code object at 0x7d4b55343fa0>, path='Results_ECCT/POLAR__Code_n_64_k_32__27_06_2024_16_10_59')\n",
            "Self-Attention Sparsity Ratio=99.00%, Self-Attention Complexity Ratio=0.50%\n",
            "Mask:\n",
            " tensor([[[[False,  True,  True,  ...,  True,  True,  True],\n",
            "          [ True, False,  True,  ...,  True,  True,  True],\n",
            "          [ True,  True, False,  ...,  True,  True,  True],\n",
            "          ...,\n",
            "          [ True,  True,  True,  ..., False,  True,  True],\n",
            "          [ True,  True,  True,  ...,  True, False,  True],\n",
            "          [ True,  True,  True,  ...,  True,  True, False]]]])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'no' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-a80432155547>\u001b[0m in \u001b[0;36m<cell line: 69>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mecct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-a80432155547>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, batch_size, iters)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# NOISELESS Channel to get (n_ldpc,1) original llrs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mllr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdemapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# no noise # (n,1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mllr_noiseless\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# decoder turns (n,1) to (n_ldpc,1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'no' is not defined"
          ]
        }
      ]
    }
  ]
}